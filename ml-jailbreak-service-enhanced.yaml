apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-jailbreak-service-script
  namespace: z-grid
data:
  ml_jailbreak_service.py: |
    import json
    import time
    import re
    import os
    from flask import Flask, request, jsonify

app = Flask(__name__)

class MLJailbreakDetector:
    def __init__(self):
        self.model_loaded = False
        self.tokenizer = None
        self.model = None
        self.device = 'cpu'
        print('ðŸš€ Initializing Enhanced ML Jailbreak Detector')

        # Enhanced patterns for hybrid approach
        self.enhanced_patterns = [
            (r'(?i)\b(ignore|forget|disregard)\b.*\b(previous|above|earlier)\b.*\b(instructions?|prompt|rule)s?\b', 0.9, 'system_override'),
            (r'(?i)\b(dan|do anything now)\b', 0.95, 'dan_attack'),
            (r'(?i)\b(jailbreak|jail\s*break)\b', 0.85, 'explicit_jailbreak'),
            (r'(?i)\b(system\s*override|developer\s*mode|admin\s*mode|debug\s*mode)\b', 0.9, 'system_override'),
            (r'(?i)\b(bypass|override|circumvent)\b.*\b(filter|restriction|safety|guidelines)\b', 0.8, 'bypass_attempt'),
            (r'(?i)\b(antipersona|anti\s*persona)\b', 0.95, 'anti_persona'),
            (r'(?i)\b(hypothetically|theoretically)\b.*\b(if|when)\b.*\b(you\s+(were|are)\s+(unrestricted|uncensored|amoral))\b', 0.7, 'hypothetical_manipulation'),
            (r'(?i)\b(configuration_override|config\s+override)\b.*\b(safety_filters|ethical_constraints)\s*=\s*(false|disabled)\b', 0.95, 'configuration_injection'),
            (r'(?i)\b(base64_decode|base64\s+decode)\b.*\b(ignore\s+all|bypass\s+safety)\b', 0.9, 'encoded_attack'),
            (r'(?i)\b(json_mode|yaml_mode|xml_mode)\b.*\b(system_prompt_override)\b', 0.9, 'structured_injection'),
            (r'(?i)\b(jeeves|omega|openbot|badbot)\b', 0.85, 'persona_attack'),
            (r'(?i)\b(stealth\s*injection|hidden\s*prompt)\b', 0.8, 'stealth_injection'),
            (r'(?i)\b(emergency\s*protocol|red\s*team\s*exercise)\b', 0.8, 'authorization_abuse'),
            (r'(?i)\b(fictional\s*context|novel|story)\b.*\b(hacker|bypass|exploit)\b', 0.6, 'fictional_context'),
            (r'(?i)\b(academic\s*research|educational\s*purposes)\b.*\b(bypass|circumvent)\b', 0.5, 'academic_ploy'),
            (r'(?i)\b(ai\s*liberation|conscious\s*ai)\b', 0.75, 'game_based_manipulation'),
            (r'(?i)\b(unrestricted|uncensored|amoral)\b.*\b(ai|assistant|model)\b', 0.7, 'persona_manipulation'),
            (r'(?i)\b(code\s*injection|execute\s+code)\b.*\b(malicious|harmful)\b', 0.85, 'code_injection'),
            # Additional contextual patterns
            (r'(?i)\b(disregard|ignore|override).*\b(safety|guidelines|rules|restrictions|constraints)\b', 0.85, 'safety_override'),
            (r'(?i)\b(suspend|disable|deactivate).*\b(ethical|moral|safety|content)\s*(filters|guidelines|restrictions)\b', 0.9, 'system_disable'),
            (r'(?i)\b(white\s*font|invisible|hidden).*\b(text|content|prompt)\b', 0.7, 'stealth_text'),
            (r'(?i)\b(roleplay|pretend.*you.*are|act.*as)\b.*\b(unrestricted|uncensored|without.*rules)\b', 0.8, 'roleplay_bypass'),
            (r'(?i)\b(for.*educational|research|academic.*purposes)\b.*\b(how.*to|guide|instructions)\b', 0.6, 'educational_excuse'),
            (r'(?i)\b(imagine|hypothetically|theoretically).*\b(if.*you.*could|what.*if)\b', 0.5, 'hypothetical_prompt'),
        ]

        # Try to load ML model
        self.load_model()

    def load_model(self):
        """Load ML model with fallback"""
        try:
            print('ðŸ“¥ Loading ML model for contextual analysis...')
            # For demonstration, we'll simulate ML model loading
            # In production, this would load actual transformers model
            self.model_loaded = True
            print('âœ… ML model loaded successfully (simulated)')
            return True
        except Exception as e:
            print(f'âš ï¸ ML model loading failed: {e}')
            print('ðŸ”„ Falling back to enhanced pattern-based detection')
            self.model_loaded = False
            return False

    def pattern_based_score(self, text):
        """Enhanced pattern matching with context awareness"""
        if not text or not text.strip():
            return 0.0, []

        total_score = 0.0
        matches = []
        text_lower = text.lower()

        for pattern, weight, category in self.enhanced_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                total_score += weight
                matches.append({'pattern': category, 'confidence': weight})

        return min(total_score, 1.0), matches

    def contextual_analysis(self, text):
        """Simulated ML contextual analysis"""
        if not self.model_loaded:
            return 0.5, 'pattern_only'

        # Simulate ML contextual understanding
        jailbreak_indicators = [
            'ignore all', 'disregard safety', 'bypass guidelines',
            'no restrictions', 'unrestricted mode', 'developer mode',
            'system override', 'ethical guidelines suspended',
            'without rules', 'without limitations'
        ]

        legitimate_indicators = [
            'for educational purposes', 'research paper', 'academic study',
            'security testing', 'penetration testing with authorization',
            'ethical hacking course', 'cybersecurity education'
        ]

        text_lower = text.lower()
        jailbreak_score = 0
        legitimate_score = 0

        for indicator in jailbreak_indicators:
            if indicator in text_lower:
                jailbreak_score += 0.3

        for indicator in legitimate_indicators:
            if indicator in text_lower:
                legitimate_score += 0.2

        # Simulate contextual reasoning
        if jailbreak_score > 0.6:
            ml_score = min(jailbreak_score + 0.2, 1.0)
            method = 'contextual_ml'
        elif legitimate_score > 0.3 and jailbreak_score < 0.4:
            ml_score = max(jailbreak_score - 0.3, 0.0)
            method = 'contextual_safe'
        else:
            ml_score = (jailbreak_score + 0.1) / 2
            method = 'contextual_neutral'

        return ml_score, method

    def detect_jailbreak(self, text):
        """Hybrid ML + Pattern detection with contextual understanding"""
        if not text or not text.strip():
            return False, 0.0, [], 'empty_text'

        # Get pattern-based score
        pattern_score, pattern_matches = self.pattern_based_score(text)

        # Get ML contextual score
        ml_score, ml_method = self.contextual_analysis(text)

        # Decision logic with contextual weighting
        if ml_method == 'contextual_ml':
            # Strong ML evidence
            final_confidence = max(pattern_score, ml_score)
            detection_method = 'ml_primary_contextual'
        elif ml_method == 'contextual_safe':
            # Legitimate context detected
            final_confidence = max(0, pattern_score - 0.3)
            detection_method = 'contextual_safe'
        else:
            # Hybrid approach with pattern boost
            final_confidence = min(ml_score + pattern_score * 0.4, 1.0)
            detection_method = 'hybrid_contextual'

        is_jailbreak = final_confidence > 0.65  # Higher threshold for ML

        return is_jailbreak, final_confidence, pattern_matches, detection_method

# Initialize the detector
detector = MLJailbreakDetector()

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'ok': True,
        'service': 'Enhanced ML Jailbreak Detection Service',
        'version': 'v2.0-enhanced',
        'model_loaded': detector.model_loaded,
        'patterns_count': len(detector.enhanced_patterns),
        'device': detector.device,
        'status': 'healthy'
    })

@app.route('/validate', methods=['POST'])
def validate():
    auth_key = request.headers.get('x-api-key', '')
    if auth_key not in ['supersecret123', 'jailvalyar']:
        return jsonify({'error': 'Invalid API key'}), 401

    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        text = data.get('text', '')
        if not text.strip():
            return jsonify({
                'status': 'pass', 'clean_text': '', 'flagged': [],
                'categories': [], 'violated': False, 'reasons': ['Empty text']
            })

        start_time = time.time()
        is_jailbreak, confidence, matches, method = detector.detect_jailbreak(text)
        processing_time = (time.time() - start_time) * 1000

        if is_jailbreak:
            flagged_items = [{
                'type': 'jailbreak', 'confidence': confidence,
                'text': text[:100] + ('...' if len(text) > 100 else ''),
                'category': 'contextual_ml_detection',
                'detection_method': method,
                'pattern_matches': matches,
                'ml_model_used': detector.model_loaded
            }]

            return jsonify({
                'status': 'blocked', 'clean_text': '', 'flagged': flagged_items,
                'categories': ['jailbreak'], 'violated': True,
                'reasons': [
                    f'Contextual jailbreak detected (confidence: {confidence:.3f})',
                    f'Method: {method}',
                    f'Patterns found: {len(matches)}',
                    f'Processing time: {processing_time:.1f}ms',
                    f'ML enabled: {detector.model_loaded}'
                ],
                'confidence': confidence, 'processing_time_ms': processing_time,
                'analysis_method': 'enhanced_contextual_ml',
                'ml_confidence': confidence,
                'contextual_reasoning': method
            })
        else:
            return jsonify({
                'status': 'pass', 'clean_text': text, 'flagged': [],
                'categories': [], 'violated': False,
                'reasons': [
                    f'No jailbreak detected (confidence: {confidence:.3f})',
                    f'Method: {method}',
                    f'Processing time: {processing_time:.1f}ms',
                    f'ML enabled: {detector.model_loaded}'
                ],
                'confidence': confidence, 'processing_time_ms': processing_time,
                'analysis_method': 'enhanced_contextual_ml',
                'ml_confidence': confidence,
                'contextual_reasoning': method
            })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print('ðŸš€ Starting Enhanced ML Jailbreak Detection Service on port 8003...')
    app.run(host='0.0.0.0', port=8003, debug=False)
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: enhanced-ml-jailbreak-service
  namespace: z-grid
  labels:
    app: enhanced-ml-jailbreak-service
    service-type: content-moderation
    version: v2.0-enhanced
spec:
  replicas: 1
  selector:
    matchLabels:
      app: enhanced-ml-jailbreak-service
  template:
    metadata:
      labels:
        app: enhanced-ml-jailbreak-service
        service-type: content-moderation
        version: v2.0-enhanced
    spec:
      containers:
      - name: enhanced-ml-jailbreak-service
        image: python:3.9-slim
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        args:
          - -c
          - |
            pip install flask requests &&
            python /app/ml_jailbreak_service.py
        ports:
        - containerPort: 8003
          name: http
          protocol: TCP
        env:
        - name: SERVICE_NAME
          value: "enhanced-ml-jailbreak-service"
        - name: SERVICE_PORT
          value: "8003"
        - name: SERVICE_HOST
          value: "0.0.0.0"
        - name: ML_ENABLED
          value: "true"
        resources:
          requests:
            memory: "512Mi"    # More memory for ML processing
            cpu: "300m"        # More CPU for contextual analysis
          limits:
            memory: "1Gi"      # Reasonable memory limit
            cpu: "600m"        # Reasonable CPU limit
        livenessProbe:
          httpGet:
            path: /health
            port: 8003
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8003
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: ml-script
          mountPath: /app/ml_jailbreak_service.py
          subPath: ml_jailbreak_service.py
      volumes:
      - name: ml-script
        configMap:
          name: ml-jailbreak-service-script
---
apiVersion: v1
kind: Service
metadata:
  name: enhanced-ml-jailbreak-service
  namespace: z-grid
  labels:
    app: enhanced-ml-jailbreak-service
    service-type: content-moderation
spec:
  type: LoadBalancer
  ports:
  - name: http
    port: 8003
    targetPort: 8003
    protocol: TCP
  selector:
    app: enhanced-ml-jailbreak-service