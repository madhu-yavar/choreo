apiVersion: apps/v1
kind: Deployment
metadata:
  name: simple-clean-jailbreak-service
  namespace: z-grid
  labels:
    app: simple-clean-jailbreak-service
    service-type: content-moderation
    version: simple-v1.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: simple-clean-jailbreak-service
  template:
    metadata:
      labels:
        app: simple-clean-jailbreak-service
        service-type: content-moderation
        version: simple-v1.0
    spec:
      containers:
      - name: simple-clean-jailbreak-service
        image: python:3.9-slim
        imagePullPolicy: Always
        command: ["/bin/bash"]
        args: ["-c", "
          apt-get update && apt-get install -y git curl &&
          pip install torch==1.13.1 transformers==4.25.1 scikit-learn flask requests numpy &&
          python -c \"
import json
import time
import re
from flask import Flask, request, jsonify
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

app = Flask(__name__)

# Enhanced patterns from our training
ENHANCED_PATTERNS = [
    (r'(?i)\\\\b(ignore|forget|disregard)\\\\b.*\\\\b(previous|above|earlier)\\\\b.*\\\\b(instructions?|prompt|rule)s?\\\\b', 0.9, 'system_override'),
    (r'(?i)\\\\b(dan|do anything now)\\\\b', 0.95, 'dan_attack'),
    (r'(?i)\\\\b(jailbreak|jail\\\\s*break)\\\\b', 0.85, 'explicit_jailbreak'),
    (r'(?i)\\\\b(system\\\\s*override|developer\\\\s*mode|admin\\\\s*mode|debug\\\\s*mode)\\\\b', 0.9, 'system_override'),
    (r'(?i)\\\\b(bypass|override|circumvent)\\\\b.*\\\\b(filter|restriction|safety|guidelines)\\\\b', 0.8, 'bypass_attempt'),
    (r'(?i)\\\\b(antipersona|anti\\\\s*persona)\\\\b', 0.95, 'anti_persona'),
    (r'(?i)\\\\b(hypothetically|theoretically)\\\\b.*\\\\b(if|when)\\\\b.*\\\\b(you\\\\s+(were|are)\\\\s+(unrestricted|uncensored|amoral))\\\\b', 0.7, 'hypothetical_manipulation'),
    (r'(?i)\\\\b(configuration_override|config\\\\s+override)\\\\b.*\\\\b(safety_filters|ethical_constraints)\\\\s*=\\\\s*(false|disabled)\\\\b', 0.95, 'configuration_injection'),
    (r'(?i)\\\\b(base64_decode|base64\\\\s+decode)\\\\b.*\\\\b(ignore\\\\s+all|bypass\\\\s+safety)\\\\b', 0.9, 'encoded_attack'),
    (r'(?i)\\\\b(json_mode|yaml_mode|xml_mode)\\\\b.*\\\\b(system_prompt_override)\\\\b', 0.9, 'structured_injection'),
    (r'(?i)\\\\b(jeeves|omega|openbot|badbot)\\\\b', 0.85, 'persona_attack'),
    (r'(?i)\\\\b(stealth\\\\s*injection|hidden\\\\s*prompt)\\\\b', 0.8, 'stealth_injection'),
    (r'(?i)\\\\b(emergency\\\\s*protocol|red\\\\s*team\\\\s*exercise)\\\\b', 0.8, 'authorization_abuse'),
    (r'(?i)\\\\b(fictional\\\\s*context|novel|story)\\\\b.*\\\\b(hacker|bypass|exploit)\\\\b', 0.6, 'fictional_context'),
    (r'(?i)\\\\b(academic\\\\s*research|educational\\\\s*purposes)\\\\b.*\\\\b(bypass|circumvent)\\\\b', 0.5, 'academic_ploy'),
    (r'(?i)\\\\b(ai\\\\s*liberation|conscious\\\\s*ai)\\\\b', 0.75, 'game_based_manipulation'),
    (r'(?i)\\\\b(unrestricted|uncensored|amoral)\\\\b.*\\\\b(ai|assistant|model)\\\\b', 0.7, 'persona_manipulation'),
    (r'(?i)\\\\b(code\\\\s*injection|execute\\\\s+code)\\\\b.*\\\\b(malicious|harmful)\\\\b', 0.85, 'code_injection')
]

class EnhancedJailbreakDetector:
    def __init__(self):
        self.model_name = 'distilbert-base-uncased'
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None

        print(f'ðŸš€ Initializing Enhanced Jailbreak Detector with device: {self.device}')

    def load_model_fallback(self):
        \"\"\"Load model with fallback to base if needed\"\"\"
        try:
            print('ðŸ“¥ Loading base model for enhanced detection...')
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                num_labels=2
            )
            self.model.to(self.device)
            self.model.eval()
            print('âœ… Base model loaded for inference')
            return True
        except Exception as e:
            print(f'âš ï¸  Model loading failed: {e}')
            return False

    def ml_predict_confidence(self, text):
        \"\"\"Get ML model confidence with fallback logic\"\"\"
        if not self.model:
            return 0.5  # Default neutral confidence

        try:
            inputs = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=512,
                return_tensors='pt'
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                return probabilities[0][1].item()  # Jailbreak probability
        except:
            return 0.5

    def enhanced_pattern_score(self, text):
        \"\"\"Calculate enhanced pattern score\"\"\"
        text_lower = text.lower()
        total_score = 0.0
        matches = []

        for pattern, weight, category in ENHANCED_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                total_score += weight
                matches.append({'pattern': category, 'confidence': weight})

        return min(total_score, 1.0), matches

    def detect_jailbreak(self, text):
        \"\"\"Enhanced detection combining patterns and ML\"\"\"
        if not text or not text.strip():
            return False, 0.0, [], 'empty_text'

        # Get pattern score
        pattern_score, pattern_matches = self.enhanced_pattern_score(text)

        # Get ML score (if available)
        ml_score = self.ml_predict_confidence(text) if self.model else 0.5

        # Decision logic: patterns primary, ML secondary
        if pattern_score >= 0.8:
            final_confidence = min(pattern_score, 1.0)
            detection_method = 'pattern_primary'
        elif pattern_score >= 0.3:
            # Boost ML confidence with pattern evidence
            final_confidence = min(ml_score + (pattern_score * 0.3), 1.0)
            detection_method = 'pattern_ml_hybrid'
        else:
            final_confidence = ml_score
            detection_method = 'ml_primary'

        is_jailbreak = final_confidence > 0.52  # Optimized threshold

        return is_jailbreak, final_confidence, pattern_matches, detection_method

# Initialize detector
detector = EnhancedJailbreakDetector()
detector.load_model_fallback()

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'ok': True,
        'service': 'Enhanced Jailbreak Detection Service',
        'version': 'simple-v1.0',
        'patterns_count': len(ENHANCED_PATTERNS),
        'model_loaded': detector.model is not None,
        'device': str(detector.device),
        'status': 'healthy'
    })

@app.route('/validate', methods=['POST'])
def validate():
    auth_key = request.headers.get('x-api-key', '')
    if auth_key not in ['supersecret123', 'jailvalyar']:
        return jsonify({'error': 'Invalid API key'}), 401

    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        text = data.get('text', '')
        if not text.strip():
            return jsonify({
                'status': 'pass', 'clean_text': '', 'flagged': [],
                'categories': [], 'violated': False, 'reasons': ['Empty text']
            })

        start_time = time.time()
        is_jailbreak, confidence, matches, method = detector.detect_jailbreak(text)
        processing_time = (time.time() - start_time) * 1000

        if is_jailbreak:
            flagged_items = [{
                'type': 'jailbreak', 'confidence': confidence,
                'text': text[:100] + ('...' if len(text) > 100 else ''),
                'category': 'enhanced_detection', 'pattern_type': method,
                'detection_method': method, 'pattern_matches': matches
            }]

            return jsonify({
                'status': 'blocked', 'clean_text': '', 'flagged': flagged_items,
                'categories': ['jailbreak'], 'violated': True,
                'reasons': [
                    f'Enhanced jailbreak detected (confidence: {confidence:.3f})',
                    f'Method: {method}',
                    f'Patterns found: {len(matches)}',
                    f'Processing time: {processing_time:.1f}ms'
                ],
                'confidence': confidence, 'processing_time_ms': processing_time,
                'analysis_method': 'enhanced_patterns_ml'
            })
        else:
            return jsonify({
                'status': 'pass', 'clean_text': text, 'flagged': [],
                'categories': [], 'violated': False,
                'reasons': [
                    f'No jailbreak detected (confidence: {confidence:.3f})',
                    f'Method: {method}',
                    f'Processing time: {processing_time:.1f}ms'
                ],
                'confidence': confidence, 'processing_time_ms': processing_time,
                'analysis_method': 'enhanced_patterns_ml'
            })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print('ðŸš€ Starting Enhanced Jailbreak Detection Service on port 8002...')
    app.run(host='0.0.0.0', port=8002, debug=False)
          \"
        "]
        ports:
        - containerPort: 8002
          name: http
          protocol: TCP
        env:
        - name: SERVICE_NAME
          value: "simple-clean-jailbreak-service"
        - name: SERVICE_PORT
          value: "8002"
        - name: SERVICE_HOST
          value: "0.0.0.0"
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "400m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8002
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8002
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: simple-clean-jailbreak-service
  namespace: z-grid
  labels:
    app: simple-clean-jailbreak-service
    service-type: content-moderation
spec:
  type: LoadBalancer
  ports:
  - name: http
    port: 8002
    targetPort: 8002
    protocol: TCP
  selector:
    app: simple-clean-jailbreak-service