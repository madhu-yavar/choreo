apiVersion: apps/v1
kind: Deployment
metadata:
  name: gibberish-service-k8s-native
  namespace: z-grid
  labels:
    app: gibberish-service-k8s-native
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gibberish-service-k8s-native
  template:
    metadata:
      labels:
        app: gibberish-service-k8s-native
    spec:
      # Init container to download PyTorch and model once
      initContainers:
      - name: pytorch-downloader
        image: python:3.11-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "üöÄ Setting up PyTorch environment..."

          # Install PyTorch and dependencies in shared volume
          pip install --no-cache-dir torch>=2.0 transformers>=4.30 flask>=2.3 datasets>=2.0 scikit-learn>=1.0

          echo "‚úÖ PyTorch installation completed"

          # Create app structure in shared volume
          mkdir -p /shared/app

          # Copy application code from ConfigMap (mounted as read-only)
          cp /config/app.py /shared/app/
          cp /config/inetuned_gibbrish_detector.py /shared/app/

          # Set proper permissions
          chmod +x /shared/app/*.py

          echo "‚úÖ Application setup completed"
          ls -la /shared/app/
        volumeMounts:
        - name: shared-volume
          mountPath: /shared
        - name: config-volume
          mountPath: /config

      containers:
      - name: gibberish-service
        image: python:3.11-slim
        # Reuse PyTorch installed by init container
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "üöÄ Starting Inetuned Gibbrish Detection service..."

          # Use PyTorch from shared volume
          export PYTHONPATH=/shared:$PYTHONPATH
          export MODEL_PATH=/model_volume/inetuned_gibbrish_model

          # Set up Python environment to use shared packages
          cd /shared/app

          # Verify PyTorch is available
          python -c "import torch; print(f'PyTorch version: {torch.__version__}')"

          # Start the service
          python app.py
        env:
        - name: MODEL_PATH
          value: "/model_volume/inetuned_gibbrish_model"
        - name: FLASK_RUN_PORT
          value: "8007"
        - name: PYTHONPATH
          value: "/shared"
        ports:
        - containerPort: 8007
          name: http
          protocol: TCP

        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"

        volumeMounts:
        - name: shared-volume
          mountPath: /shared
        - name: model-volume
          mountPath: /model_volume
          readOnly: true

        livenessProbe:
          httpGet:
            path: /health
            port: 8007
          initialDelaySeconds: 180  # Increased for PyTorch download
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /health
            port: 8007
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

      volumes:
      - name: shared-volume
        emptyDir:
          sizeLimit: 4Gi  # For PyTorch and app files

      - name: model-volume
        persistentVolumeClaim:
          claimName: gibberish-model-pvc

      - name: config-volume
        configMap:
          name: gibberish-service-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gibberish-service-config
  namespace: z-grid
data:
  app.py: |
    #!/usr/bin/env python3
    import os
    import time
    from flask import Flask, request, jsonify
    from inetuned_gibbrish_detector import InetunedGibbrishDetector

    app = Flask(__name__)
    detector = None

    def load_model():
        global detector
        try:
            print('üöÄ Loading Inetuned Gibbrish Model...')
            model_path = os.environ.get('MODEL_PATH', '/model_volume/inetuned_gibbrish_model')
            print(f'üìÇ Model path: {model_path}')

            detector = InetunedGibbrishDetector(model_name=None, local_model_path=model_path)

            if not detector.is_loaded:
                print('‚ùå Failed to load model')
                return False

            print('‚úÖ Inetuned Gibbrish Model loaded successfully!')
            print('üìä Training accuracy: 96.7%')
            return True
        except Exception as e:
            print(f'‚ùå Error loading model: {e}')
            return False

    @app.route('/')
    def home():
        return jsonify({
            'service': 'Gibberish Detection API',
            'model': 'Inetuned Gibbrish Model',
            'model_version': 'v2.0',
            'training_accuracy': '96.7%',
            'deployment': 'k8s-native-init-container',
            'status': 'ready'
        })

    @app.route('/health', methods=['GET'])
    def health_check():
        return jsonify({
            'status': 'healthy' if detector and detector.is_loaded else 'unhealthy',
            'model_loaded': detector.is_loaded if detector else False,
            'model_path': os.getenv('MODEL_PATH', '/model_volume/inetuned_gibbrish_model'),
            'model_type': 'inetuned',
            'model_version': 'v2.0',
            'training_accuracy': '96.7%',
            'deployment': 'k8s-native'
        })

    @app.route('/detect', methods=['POST'])
    def detect_gibberish():
        try:
            data = request.get_json()
            if not data or 'text' not in data:
                return jsonify({'error': 'Missing text field'}), 400

            text = data['text']
            if not isinstance(text, str):
                return jsonify({'error': 'Text must be a string'}), 400

            if not detector or not detector.is_loaded:
                return jsonify({'error': 'Model not loaded'}), 503

            start_time = time.time()
            result = detector.detect(text)
            inference_time = time.time() - start_time

            result['inference_time_ms'] = round(inference_time * 1000, 2)
            result['deployment_type'] = 'k8s-native-init-container'

            return jsonify(result)

        except Exception as e:
            return jsonify({'error': str(e)}), 500

    @app.route('/model_info', methods=['GET'])
    def model_info():
        return jsonify({
            'model_type': 'inetuned',
            'model_version': 'v2.0',
            'model_path': os.getenv('MODEL_PATH', '/model_volume/inetuned_gibbrish_model'),
            'training_dataset_size': 298,
            'training_accuracy': 0.967,
            'training_f1_score': 0.944,
            'training_epochs': 10,
            'deployment_type': 'k8s-native-init-container',
            'status': 'ready'
        })

    if __name__ == '__main__':
        if not load_model():
            print('‚ùå Failed to start server - model loading failed')
            exit(1)

        port = int(os.environ.get('FLASK_RUN_PORT', 8007))
        app.run(host='0.0.0.0', port=port, debug=False)

  inetuned_gibbrish_detector.py: |
    #!/usr/bin/env python3
    """
    Inetuned Gibbrish Detector - Production version
    """
    import os
    import logging
    import re
    from typing import Dict, Any, List, Optional
    from datetime import datetime

    # These will be available after init container setup
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
        DEPENDENCIES_AVAILABLE = True
    except ImportError:
        DEPENDENCIES_AVAILABLE = False
        print("‚ö†Ô∏è  Dependencies not available - will be installed by init container")

    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    class InetunedGibbrishDetector:
        """
        Production Inetuned Gibbrish detector
        Uses fine-tuned model with 96.7% accuracy on 298 samples
        """

        def __init__(self, model_name: str = None, local_model_path: str = None):
            self.model_name = model_name or "Inetuned Gibbrish Model v2.0"
            self.local_model_path = local_model_path or "/model_volume/inetuned_gibbrish_model"
            self.tokenizer = None
            self.model = None
            self.pipeline = None
            self.is_loaded = False

            if DEPENDENCIES_AVAILABLE:
                self.load_model()
            else:
                logger.warning("Dependencies not yet installed - init container will handle this")

        def load_model(self):
            """Load the Inetuned Gibbrish model"""
            try:
                if not DEPENDENCIES_AVAILABLE:
                    logger.error("Dependencies not available - cannot load model")
                    return False

                model_path = self.local_model_path
                if os.path.exists(model_path):
                    logger.info(f"Loading Inetuned Gibbrish Model from {model_path}")
                    self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                    self.model = AutoModelForSequenceClassification.from_pretrained(model_path)

                    # Create pipeline for easy inference
                    self.pipeline = pipeline(
                        "text-classification",
                        model=self.model,
                        tokenizer=self.tokenizer,
                        device=0 if torch.cuda.is_available() else -1,
                        top_k=None
                    )

                    self.is_loaded = True
                    logger.info("‚úÖ Inetuned Gibbrish Model loaded successfully")
                else:
                    logger.error(f"‚ùå Model path not found: {model_path}")
                    self.is_loaded = False

            except Exception as e:
                logger.error(f"‚ùå Failed to load Inetuned Gibbrish Model: {e}")
                self.is_loaded = False

        def detect(self, text: str) -> Dict[str, Any]:
            """
            Detect if text is gibberish using Inetuned Gibbrish model
            """
            if not self.is_loaded or not DEPENDENCIES_AVAILABLE:
                return {
                    "is_gibberish": False,
                    "confidence": 0.0,
                    "details": "Model not loaded or dependencies missing",
                    "model_type": "inetuned_error",
                    "error": "Dependencies not available"
                }

            if not text or len(text.strip()) < 3:
                return {
                    "is_gibberish": False,
                    "confidence": 0.0,
                    "details": "Text too short for analysis",
                    "model_type": "inetuned",
                    "model_name": self.model_name
                }

            try:
                # Use pipeline for prediction
                pipeline_result = self.pipeline(text)

                # Handle the nested list structure from pipeline
                result = pipeline_result[0] if isinstance(pipeline_result, list) and len(pipeline_result) > 0 else []

                # Extract probability
                gibberish_score = None
                valid_score = None

                # Handle different label formats from training vs pre-trained models
                for score in result:
                    label = score['label'].lower()
                    if 'gibberish' in label or label == 'label_1':
                        gibberish_score = score['score']
                    elif 'clean' in label or 'valid' in label or label == 'label_0':
                        valid_score = score['score']

                # Determine if gibberish
                is_gibberish = gibberish_score > valid_score if gibberish_score and valid_score else False

                # Get confidence
                confidence = max(gibberish_score or 0, valid_score or 0)

                response = {
                    "is_gibberish": is_gibberish,
                    "confidence": float(confidence),
                    "details": "Inetuned Gibbrish Model prediction",
                    "model_type": "inetuned",
                    "model_name": self.model_name,
                    "prediction_proba": {
                        "valid": float(valid_score or 0),
                        "gibberish": float(gibberish_score or 0)
                    }
                }

                return response

            except Exception as e:
                logger.error(f"‚ùå Inetuned Gibbrish Model detection error: {e}")
                return {
                    "is_gibberish": False,
                    "confidence": 0.0,
                    "details": f"Inetuned Gibbrish Model detection error: {str(e)}",
                    "model_type": "inetuned_error",
                    "error": str(e)
                }

---
apiVersion: v1
kind: Service
metadata:
  name: gibberish-service-k8s-native
  namespace: z-grid
  labels:
    app: gibberish-service-k8s-native
spec:
  selector:
    app: gibberish-service-k8s-native
  ports:
  - name: http
    port: 8007
    targetPort: 8007
    protocol: TCP
  type: ClusterIP